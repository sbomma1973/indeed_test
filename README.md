# Indeed Elasticsearch Indexer (PySpark Version)

## Project Overview

This project contains an AWS Glue job script (`Indeed_PySpark.py`) designed to read data from a source (e.g., MongoDB), process it, and index it into an Elasticsearch cluster. It leverages PySpark for distributed data processing and the official Elasticsearch Spark connector for efficient data sinking. The script also includes an optional mode to enrich data with embeddings generated by an AWS SageMaker endpoint before indexing.

This version is an optimized PySpark implementation focusing on robust error handling, configurable batching, and timeout management for large-scale data processing.

## Features

*   **Distributed Processing:** Utilizes PySpark for scalable data ingestion and transformation.
*   **Elasticsearch Integration:** Uses the `elasticsearch-spark` connector for efficient and reliable bulk indexing to Elasticsearch.
*   **SageMaker Embedding (Optional):** Can invoke an AWS SageMaker endpoint to generate embeddings for documents before indexing.
*   **Configurable Parameters:** Job behavior is controlled through AWS Glue job parameters (e.g., source/sink details, batch sizes, date ranges, indexing mode).
*   **Dynamic Schema Handling:** Loads document schema from an S3 location.
*   **Refreshable Credentials:** Uses refreshable credentials for assuming roles, enhancing security for SageMaker interactions.
*   **Robust Error Handling:** Includes retry mechanisms and comprehensive logging.
*   **Optimized for Timeouts:** Configured with improved timeout settings for Elasticsearch and SageMaker operations.

## Prerequisites

*   Python 3.x
*   Apache Spark (compatible with the `elasticsearch-spark-30_2.12:8.11.3` connector)
*   AWS Glue environment (or a Spark environment configured to run Glue jobs)
*   Boto3 (AWS SDK for Python)
*   `elasticsearch-spark-30_2.12:8.11.3.jar` (or a later compatible version) added to your Spark session's classpath.
*   Access to AWS Services:
    *   Secrets Manager (for storing credentials)
    *   S3 (for storing source schema)
    *   Source database (e.g., MongoDB)
    *   Elasticsearch cluster
    *   SageMaker endpoint (if `WITH_EMBEDDING` mode is used)
    *   IAM roles with necessary permissions.

## Configuration

The script is configured using AWS Glue job arguments. Key arguments include:

*   `JOB_NAME`: Name of the Glue job.
*   `credentials`: Secret ID in AWS Secrets Manager for database and Elasticsearch credentials.
*   `bucket`: S3 bucket for the source schema.
*   `source_schema`: S3 key for the JSON schema file.
*   `source_db_name`, `source_db_collection`: Source database and collection details.
*   `sink_index`: Target Elasticsearch index name.
*   `states`, `countries`: Comma-separated lists for filtering data.
*   `mode`: `DEFAULT` or `WITH_EMBEDDING`.
*   `sagemaker_endpoint`: Name of the SageMaker endpoint (if `WITH_EMBEDDING` is used).
*   `sagemaker_role_arn`: IAM role ARN for SageMaker invocation.
*   `batch_size`: Batch size for processing.
*   `sagemaker_batch_size`: Batch size for SageMaker endpoint invocation.
*   `from_date`, `to_date` (Optional): Date range for filtering data.

### Spark Session Configuration (within the script):

The `create_spark_session` function configures the Spark session with necessary Elasticsearch connector settings:
```python
spark = (SparkSession.builder
    .appName("Indeed_ES_Indexer")
    .config("spark.jars.packages", "org.elasticsearch:elasticsearch-spark-30_2.12:8.11.3")
    .config("es.nodes", sink_creds['endpoint'].split('://')[1])
    .config("es.port", "443")
    .config("es.nodes.wan.only", "true")
    # ... other ES configurations ...
    .getOrCreate())
```

## Running the Script

This script is intended to be run as an AWS Glue job.

1.  **Package Dependencies:** Ensure the `elasticsearch-spark` JAR is available to your Glue job (e.g., via `--extra-jars` S3 path or by including it in a custom worker).
2.  **Set Up IAM Roles:** The Glue job role needs permissions to access Secrets Manager, S3, the source database, Elasticsearch, and SageMaker (if applicable). The `sagemaker_role_arn` also needs appropriate SageMaker invocation permissions.
3.  **Configure Glue Job:**
    *   Create a new Glue job or modify an existing one.
    *   Set the script location to `Indeed_PySpark.py`.
    *   Provide the necessary job parameters (see Configuration section).
    *   Configure job properties like worker type, number of workers, and timeouts.
4.  **Run the Job.**

## Code Structure (`Indeed_PySpark.py`)

*   **Argument Parsing:** `get_glue_args` function for handling job parameters.
*   **Spark Session Initialization:** `create_spark_session` for setting up Spark with Elasticsearch connector.
*   **AWS Service Initialization:** Sets up Boto3 clients for Secrets Manager, S3, STS.
*   **Credential & Schema Loading:** Fetches credentials and schema.
*   **SageMaker Integration:**
    *   `refreshable_assumed_role_session`: Manages STS assumed role credentials for SageMaker.
    *   `process_with_embeddings`: Orchestrates fetching embeddings from SageMaker.
    *   `map_resume`: Transforms data into the format expected by the SageMaker endpoint.
*   **Data Transformation:**
    *   `prepare_document`: Prepares individual documents for Elasticsearch by removing specified fields.
    *   Helper functions (`map_skill`, `map_experience`, `map_education`) for structuring document sub-fields.
*   **Main Logic:**
    *   `reindex_documents`: Core function that reads from the source, applies filters, optionally fetches embeddings, prepares data, and writes to Elasticsearch.
*   **Entry Point:** `if __name__ == "__main__":` block handles job initialization and execution.

## Error Handling and Timeout Management

*   The Elasticsearch Spark connector provides built-in retry mechanisms (`es.batch.write.retry.count`, `es.batch.write.retry.wait`).
*   The script includes `try-except` blocks for robust error handling and logging.
*   SageMaker invocation includes basic error logging.
*   Elasticsearch connection parameters (`es.http.timeout`, `es.http.retries`) are configured in the Spark session.

## To-Do / Potential Improvements

*   More sophisticated error handling for SageMaker API calls (e.g., retries with backoff).
*   Dead-letter queue for failed documents.
*   Metrics for monitoring indexing progress and failures (e.g., using Spark accumulators or custom CloudWatch metrics).
*   More dynamic batch sizing based on document characteristics.